{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO4PqkUWPH9/Y2sCr0KFTE4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ieXqdB1-JrKq","executionInfo":{"status":"ok","timestamp":1755757033954,"user_tz":-330,"elapsed":363,"user":{"displayName":"MERUVA REDDY GEETHA NARASIMHA,CSE(2022) Vel Tech, Chennai","userId":"05886514978054344328"}},"outputId":"ab2aae96-e37b-4ef8-86ca-fdcb53bba80c"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Morphology of the document:\n","text: 3\n","words: 3\n","process: 2\n","example: 1\n","document: 1\n","used: 1\n","demonstrate: 1\n","without: 1\n","loading: 1\n","file: 1\n"]}],"source":["import nltk\n","import tempfile\n","import os\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.probability import FreqDist\n","\n","# Set NLTK data path to a temporary directory\n","temp_dir = tempfile.mkdtemp()\n","nltk.data.path.append(temp_dir)\n","\n","nltk.download('punkt', download_dir=temp_dir)\n","nltk.download('stopwords', download_dir=temp_dir)\n","nltk.download('punkt_tab', download_dir=temp_dir) # Download the missing resource\n","\n","def tokenize_document(document):\n","    tokens = word_tokenize(document)\n","    return [word.lower() for word in tokens if word.isalpha()]\n","def remove_stopwords(tokens):\n","    stop_words = set(stopwords.words('english'))\n","    return [word for word in tokens if word not in stop_words]\n","def find_morphology(tokens):\n","    fdist = FreqDist(tokens)\n","    return fdist.most_common(10)\n","# Document data provided as a string instead of from a file\n","document_text = \"\"\"\n","This is an example document. It's used to demonstrate how to process text\n","without loading it from a file. We will analyze the words in this text,\n","tokenize them, remove common words like 'is' and 'a', and find the most\n","frequent words. The process is straightforward and helps us understand the\n","basic structure of the text. \"\"\"\n","\n","tokens = tokenize_document(document_text)\n","tokens_without_stopwords = remove_stopwords(tokens)\n","morphology = find_morphology(tokens_without_stopwords)\n","print(\"Morphology of the document:\")\n","for word, frequency in morphology:\n","    print(f\"{word}: {frequency}\")\n","\n","# Clean up the temporary directory (optional)\n","# import shutil\n","# shutil.rmtree(temp_dir)"]}]}